{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Methode :\n",
    "- **Support Vector Machine (SVM)** : Un algorithme de classification qui trouve l'hyperplan optimal dans un espace de grande dimension pour séparer les différentes classes. Il peut également être étendu pour gérer des problèmes non linéaires en utilisant des noyaux.\n",
    "\n",
    "- **Régression logistique** : Un algorithme utilisé pour la classification binaire (et pouvant être étendu à la classification multiclasse) en modélisant la probabilité que chaque classe soit la classe cible à l'aide d'une fonction logistique.\n",
    "- **Random Forest** : Un algorithme d'ensemble utilisé pour la classification et la régression. Il combine les prédictions de plusieurs arbres de décision pour obtenir une prédiction plus robuste et généralement de meilleure qualité.\n",
    "- **Réseaux de neurones** :\n",
    "- **Perceptron ou Multi-perceptron** :\n",
    "- **Gradient Boosting** : Un autre algorithme d'ensemble qui construit des arbres de décision de manière séquentielle, en corrigeant les erreurs des arbres précédents. Cela conduit à un modèle de prédiction puissant.\n",
    "- **Naive Bayes** : Un classificateur probabiliste simple basé sur le théorème de Bayes avec une forte indépendance entre les fonctionnalités. Il est souvent utilisé pour la classification de texte et d'autres tâches où l'indépendance des fonctionnalités est une hypothèse raisonnable.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15caaa19a1d974d5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-06T16:54:41.568918Z",
     "start_time": "2024-04-06T16:54:41.565280Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder ,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_dir_path = '../src/data/'\n",
    "data_trainset_cleaned = pd.read_csv(data_dir_path + \"/data_trainset_cleaned.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T16:54:44.208645Z",
     "start_time": "2024-04-06T16:54:44.185726Z"
    }
   },
   "id": "6e916a0db3fc6cbf",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing data per column after imputation:\n",
      "species      0.0\n",
      "margin1      0.0\n",
      "margin2      0.0\n",
      "margin3      0.0\n",
      "margin4      0.0\n",
      "            ... \n",
      "texture60    0.0\n",
      "texture61    0.0\n",
      "texture62    0.0\n",
      "texture63    0.0\n",
      "texture64    0.0\n",
      "Length: 187, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a copy to avoid modifying the original DataFrame\n",
    "dataset_encoded = data_trainset_cleaned.copy()\n",
    "\n",
    "# Encoding the 'species' column\n",
    "encoder = LabelEncoder()\n",
    "dataset_encoded['species'] = encoder.fit_transform(dataset_encoded['species'])\n",
    "\n",
    "# Select numeric columns except for the already encoded 'species' column for imputation\n",
    "numeric_columns = dataset_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize the KNNImputer instance\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Apply imputation only on the numeric columns\n",
    "dataset_encoded[numeric_columns] = imputer.fit_transform(dataset_encoded[numeric_columns])\n",
    "\n",
    "# Convert the NumPy array returned by KNNImputer back into a pandas DataFrame and match column names\n",
    "trainset_imputed = pd.DataFrame(dataset_encoded, columns=dataset_encoded.columns)\n",
    "\n",
    "# Calculate the percentage of NaN values for each column after imputation\n",
    "na_percentage = trainset_imputed.isna().mean() * 100\n",
    "\n",
    "# Display the percentages\n",
    "print(\"Percentage of missing data per column after imputation:\")\n",
    "print(na_percentage)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T16:54:47.578954Z",
     "start_time": "2024-04-06T16:54:46.866638Z"
    }
   },
   "id": "844cbd64464a7319",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mixing and Splitting of Data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232ac01a61972037"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Shuffle the new dataset\n",
    "new_dataset_shuffled = shuffle(trainset_imputed)\n",
    "\n",
    "# Separate target from data\n",
    "features = trainset_imputed.drop(\"species\", axis=1)\n",
    "new_target = trainset_imputed['species']\n",
    "\n",
    "# Create training set and testing set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, new_target, test_size=0.2, stratify=new_target\n",
    ")\n",
    "\n",
    "# Standardization of the test set\n",
    "scaler = StandardScaler()\n",
    "features_train_standardized = scaler.fit_transform(features_train)\n",
    "features_test_standardized = scaler.transform(features_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T17:59:11.274568Z",
     "start_time": "2024-04-06T17:59:11.244230Z"
    }
   },
   "id": "4a1a8430e9211d65",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrainement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f263bee9c8bdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DecisionTree\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2735fcdf762e4ae"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Meilleurs hyperparamètres: {'criterion': 'entropy', 'max_depth': 10}\n",
      "Accuracy: 0.6868686868686869\n",
      "F1 Score: 0.6611351611351611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Définir les hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 10, None]\n",
    "}\n",
    "\n",
    "# Créer l'instance de DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Configurer GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Exécuter la recherche en grille sur les données d'entraînement standardisées\n",
    "grid_search.fit(features_train_standardized, target_train)\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "predictions = best_model.predict(features_test_standardized)\n",
    "\n",
    "# Évaluer la performance\n",
    "accuracy = accuracy_score(target_test, predictions)\n",
    "f1 = f1_score(target_test, predictions, average='macro')  # ou 'macro' pour une classification multiclasse\n",
    "\n",
    "print(f\"Meilleurs hyperparamètres: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T18:16:44.194371Z",
     "start_time": "2024-04-06T18:16:41.634363Z"
    }
   },
   "id": "c2599632386510e6",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores de précision de validation croisée : 0.6729878194411273\n",
      "Moyenne des scores F1 de validation croisée : 0.6224883758217092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Calculer les scores de validation croisée\n",
    "cv_scores_accuracy = cross_val_score(best_model, features_train_standardized, target_train, cv=5, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(best_model, features_train_standardized, target_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Afficher les scores moyens de la validation croisée\n",
    "print(f\"Moyenne des scores de précision de validation croisée : {np.mean(cv_scores_accuracy)}\")\n",
    "print(f\"Moyenne des scores F1 de validation croisée : {np.mean(cv_scores_f1)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T18:16:47.789608Z",
     "start_time": "2024-04-06T18:16:46.405155Z"
    }
   },
   "id": "da12b69596e6d6b8",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LogisticRegression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65dee8453fdf497f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Meilleurs hyperparamètres pour Logistic Regression: {'C': 0.01, 'solver': 'liblinear'}\n",
      "Accuracy pour Logistic Regression: 0.9949494949494949\n",
      "F1 Score pour Logistic Regression: 0.9946127946127946\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_lr = {\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'C': np.logspace(-3, 3, 7)\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, penalty='l2', random_state=42)\n",
    "grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Assurez-vous d'avoir déjà standardisé features_train et features_test\n",
    "grid_search_lr.fit(features_train_standardized, target_train)\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "# Prédictions sur l'ensemble de test standardisé\n",
    "predictions_lr = best_model_lr.predict(features_test_standardized)\n",
    "\n",
    "# Évaluation de la performance\n",
    "accuracy_lr = accuracy_score(target_test, predictions_lr)\n",
    "f1_lr = f1_score(target_test, predictions_lr, average='macro')\n",
    "\n",
    "print(f\"Meilleurs hyperparamètres pour Logistic Regression: {grid_search_lr.best_params_}\")\n",
    "print(f\"Accuracy pour Logistic Regression: {accuracy_lr}\")\n",
    "print(f\"F1 Score pour Logistic Regression: {f1_lr}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T18:17:04.904752Z",
     "start_time": "2024-04-06T18:16:51.969013Z"
    }
   },
   "id": "74619ff49dedf772",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores de précision de validation croisée pour Logistic Regression: 0.9962184539447495\n",
      "Moyenne des scores F1 de validation croisée pour Logistic Regression: 0.9946127946127945\n"
     ]
    }
   ],
   "source": [
    "# Calculer les scores de validation croisée pour la précision\n",
    "cv_scores_accuracy_lr = cross_val_score(best_model_lr, features_train_standardized, target_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Calculer les scores de validation croisée pour le score F1\n",
    "cv_scores_f1_lr = cross_val_score(best_model_lr, features_train_standardized, target_train, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Afficher les scores moyens de la validation croisée\n",
    "print(f\"Moyenne des scores de précision de validation croisée pour Logistic Regression: {np.mean(cv_scores_accuracy_lr)}\")\n",
    "print(f\"Moyenne des scores F1 de validation croisée pour Logistic Regression: {np.mean(cv_scores_f1_lr)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T18:17:16.773482Z",
     "start_time": "2024-04-06T18:17:07.887939Z"
    }
   },
   "id": "4a4d7c309599c9",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "487fcb09b41b5bde"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
